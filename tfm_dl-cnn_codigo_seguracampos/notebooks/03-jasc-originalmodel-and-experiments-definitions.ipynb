{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b1680e-ee02-47a1-a4a6-2fb565451674",
   "metadata": {},
   "source": [
    "# Original Experiment Model\n",
    "In section 4 of their paper, (E. López-Jiménez, et al., 2019), defined the model for the Convolutional Neural Network they were going to use for the cactus recognition. They defined the model as modified version of LeNet-5, a network originally proposed by LeCun et al. (LeCun et al., 1998). This modified version was denoted by the greek letter $\\Phi$.\n",
    "\n",
    "The model was \"configured as follows. It receives an input 3-channel image with resolution 32x32x32. Then, 6 convolution filters with size 5x5 are applied with stride one. Next, a max pooling operation is performed, using a kernel of 2x2 and stride two. Then, a new set of 16 convolution kernels with size 5x5 and stride one is applied. Again a max pooling operation is performed using a kernel of 2x2 and stride two, Next, the features are flattened to a one dimension vector of size 400. Later, three fully connected layers are applied with 120, 84, and 2 nodes respectively. Until this point, the output of the CNN is a vector of real numbers called logits. Therefore, a *LogSoftMax* function is applied to convert the logits into a normalized probability distribution\"(E. López-Jiménez, et al., 2019).\n",
    "\n",
    "Using Python with Pytorch library, the model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d58e842-3308-4b6e-b78e-0f4057c95457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jascrer/anaconda3/envs/TFM/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class PhiLetnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The custom LetNet-5 CNN described in the article, defined with the greek letter PHI\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride= 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride= 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=400, out_features=120),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Linear(in_features=84, out_features=2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_parameter):\n",
    "        \"\"\"\n",
    "        Data processing method\n",
    "        \"\"\"\n",
    "        x_parameter = self.features(x_parameter)\n",
    "        x_parameter = self.flatten(x_parameter)\n",
    "        x_parameter = self.classifier(x_parameter)\n",
    "        return x_parameter\n",
    "\n",
    "phi_model = PhiLetnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae87ce4-541a-4d6e-a572-3dc4edc5d1e5",
   "metadata": {},
   "source": [
    "The class was named PhiLetNet as it is the customized LetNet denominated by the letter $\\Phi$.\n",
    "\n",
    "In the class constructor, the class defines 3 properties called features, flatten and classifier.\n",
    "\n",
    "**Features** contains the definition for the convolutional and max pooling layers.\n",
    "\n",
    "**Flatten** defines the flattening layer specified before the fully connected layers.\n",
    "\n",
    "**Classifier** contains the three fully connected layers with the specified inputs and lastly a LogSoftmax function that is applied to the output of the fully connected layers to provide the normalized probability distribution as a result like in the definition of E. López-Jiménez, et al.\n",
    "\n",
    "The *forward* method contains the flow in which the inputs will be processed:\n",
    "* First, the inputs are processed by the convolutional and max pooling layers (features)\n",
    "* Then, the output will be flatten to then\n",
    "* Be fed to the classifier (fully connected layers) to determine the category they will be in.\n",
    "\n",
    "## Training configuration\n",
    "For model training, E. López-Jiménez, et al. proposed the use of the Adam optimizer, which is \"a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement\" (Kingma and Ba, 2015). Adam is a great alternative to the Stochastic Gradient Decent algorithm.\n",
    "### Differences in equipment\n",
    "In the paper, E. López-Jiménez, et al. specified that they used an Intel i7 machine with NVIDIA GeForce 1080 GPU. \n",
    "\n",
    "But in the case of this project, the equipment used for the execution of the experiments is an Intel Quad Core i7 with a NVIDIA GeForce GTX 960M.\n",
    "\n",
    "During the execution of the experiments, the equipment did not showed any issue in performance or latency, but it should be *noted* that the original machine has a more capable GPU.\n",
    "### Hyperparameters\n",
    "E. López-Jiménez, et al. specified the learning rate, epoch count and batch size for the experiment, which are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae0a2a1-d702-45af-b8a5-87df7a47678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCH_COUNT = 150\n",
    "BATCH_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fca81-6e2d-475b-8f23-e26d07d56f34",
   "metadata": {},
   "source": [
    "Based in the paper specifications, the Adam optimizer can be defined from Pytorch by using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7afd43e-7b5c-4b72-9145-3cb162f08e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam(phi_model.parameters(), LEARNING_RATE)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b79c-7ac0-4928-8d3f-9cb16297d0fb",
   "metadata": {},
   "source": [
    "For training, E. López-Jiménez, et al. defined the Negative Logarithmic Likelihood function as the loss function. On Pytorch the definition would be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1874c0-9b52-4690-b122-df21a57eb9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab9d5e-4c4b-46de-8d20-6ef8d5e5a3a9",
   "metadata": {},
   "source": [
    "## Execution Times\n",
    "This model has execution times between 15 and 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c439c2-30af-4e02-a901-b9e85ce2d84e",
   "metadata": {},
   "source": [
    "# References\n",
    "* López-Jiménez, Efren; Vasquez-Gomez, Juan Irving; Sanchez-Acevedo, Miguel Angel; Herrera-Lozada, Juan Carlos; Uriarte-Arcia, Abril Valeria (2019); “Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach”. Ecological Informatics. 52. 131-138.\n",
    "* LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86 (11), 2278–2324.\n",
    "* Kingma, D.P., Ba, J., 2015. Adam: A Method for Stochastic Optimization. In: Proceedings of the 3rd International Conference on Learning Representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
